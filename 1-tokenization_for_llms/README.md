## Objective: Learn about tokenizers and why they are essential in Generative AI (artificial intelligence).

Basic elements before training a generative model.
An index to the main topics covered in this unit:

2.1 Database

2.2 Tokenize

2.2.1 Byte-Pair Encoding (BPE) tokenization
2.2.2 WordPiece tokenization
2.2.3 Unigram tokenization
2.2.4 SentencePiece tokenization
2.2.5 How to use the Hugging Face tokenizers library
2.2.5 How to train and deploy your own tokenizers to Hugging Face


## Useful Links